# Environment variable configuration for the RAG backend

# --- Provider Selection (Optional - Defaults shown) ---
# EMBEDDING_PROVIDER="openai" # Options: openai, azure, ollama, bedrock, huggingface, lmstudio (default)
# LLM_PROVIDER="openai"      # Options: openai, azure, ollama, bedrock, lmstudio

# --- OpenAI Configuration ---
# OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

# --- Azure OpenAI Configuration ---
# AZURE_OPENAI_API_KEY="YOUR_AZURE_API_KEY"
# AZURE_OPENAI_ENDPOINT="YOUR_AZURE_ENDPOINT" # e.g., https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION="YYYY-MM-DD" # e.g., 2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME="YOUR_EMBEDDING_DEPLOYMENT_NAME" # e.g., text-embedding-ada-002
# AZURE_OPENAI_LLM_DEPLOYMENT_NAME="YOUR_LLM_DEPLOYMENT_NAME" # e.g., gpt-35-turbo

# --- AWS Bedrock Configuration ---
# Authentication: Provide credentials via ONE OF the following methods:
# 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, optionally AWS_SESSION_TOKEN)
# 2. AWS credentials profile (BEDROCK_PROFILE_NAME)
# 3. IAM role attached to the execution environment (if running on EC2, ECS, Lambda, etc.)
# AWS_REGION="YOUR_AWS_REGION" # e.g., us-east-1
# BEDROCK_EMBEDDING_MODEL_ID="YOUR_BEDROCK_EMBEDDING_MODEL_ID" # e.g., amazon.titan-embed-text-v1
# BEDROCK_MODEL_ID="YOUR_BEDROCK_LLM_MODEL_ID" # e.g., anthropic.claude-v2
# --- Auth Method 1: Access Keys (Uncomment if using this method) ---
# AWS_ACCESS_KEY_ID="YOUR_AWS_ACCESS_KEY_ID"
# AWS_SECRET_ACCESS_KEY="YOUR_AWS_SECRET_ACCESS_KEY"
# AWS_SESSION_TOKEN="YOUR_AWS_SESSION_TOKEN" # Optional: If using temporary credentials
# --- Auth Method 2: Profile Name (Uncomment if using this method) ---
# BEDROCK_PROFILE_NAME="YOUR_AWS_PROFILE_NAME"

# --- Ollama Configuration ---
# OLLAMA_EMBEDDING_MODEL="YOUR_OLLAMA_EMBEDDING_MODEL" # e.g., nomic-embed-text:latest 
# OLLAMA_LLM_MODEL="YOUR_OLLAMA_LLM_MODEL"           # e.g., gemma3:1b
# OLLAMA_BASE_URL="http://localhost:11434" # Default if Ollama runs locally

# --- LM Studio Configuration ---
# LM_STUDIO_BASE_URL="http://localhost:1234" # Default LM Studio local server URL
# LM_STUDIO_API_KEY="lm-studio" # Default dummy API key (LM Studio doesn't require real authentication)
# LM_STUDIO_MODEL="YOUR_LM_STUDIO_MODEL" # The model name as shown in LM Studio (e.g., "meta-llama/Meta-Llama-3.1-8B-Instruct-GGUF")
# LM_STUDIO_EMBEDDING_MODEL="YOUR_LM_STUDIO_EMBEDDING_MODEL" # The embedding model name in LM Studio (if supported)

# --- RAG Configuration ---
# RETRIEVER_K=3 # Number of relevant document chunks to retrieve (default: 3)
# LLM_TEMPERATURE=0.7 # Controls randomness in LLM generation (e.g., 0.0=deterministic, 1.0=creative, default: 0.7)

# --- Advanced Configuration ---
# The system automatically detects when embedding providers/models change and rebuilds the vector database
# to prevent compatibility issues. This ensures the application remains stable when switching providers.