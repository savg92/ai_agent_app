# Environment variable configuration for the RAG backend

# --- Provider Selection (Optional - Defaults shown) ---
# EMBEDDING_PROVIDER="openai" # Options: openai, azure, ollama, bedrock, huggingface (default)
# LLM_PROVIDER="openai"      # Options: openai, azure, ollama, bedrock

# --- OpenAI Configuration ---
# OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

# --- Azure OpenAI Configuration ---
# AZURE_OPENAI_API_KEY="YOUR_AZURE_API_KEY"
# AZURE_OPENAI_ENDPOINT="YOUR_AZURE_ENDPOINT" # e.g., https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION="YYYY-MM-DD" # e.g., 2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME="YOUR_EMBEDDING_DEPLOYMENT_NAME" # e.g., text-embedding-ada-002
# AZURE_OPENAI_LLM_DEPLOYMENT_NAME="YOUR_LLM_DEPLOYMENT_NAME" # e.g., gpt-35-turbo

# --- AWS Bedrock Configuration ---
# Ensure AWS credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc.) are configured via environment or IAM role.
# AWS_REGION="YOUR_AWS_REGION" # e.g., us-east-1
# BEDROCK_EMBEDDING_MODEL_ID="YOUR_BEDROCK_EMBEDDING_MODEL_ID" # e.g., amazon.titan-embed-text-v1
# BEDROCK_MODEL_ID="YOUR_BEDROCK_LLM_MODEL_ID" # e.g., anthropic.claude-v2
# BEDROCK_PROFILE_NAME="YOUR_AWS_PROFILE_NAME" # Optional: If using a specific AWS profile

# --- Ollama Configuration ---
# OLLAMA_EMBEDDING_MODEL="YOUR_OLLAMA_EMBEDDING_MODEL" # e.g., mxbai-embed-large
# OLLAMA_LLM_MODEL="YOUR_OLLAMA_LLM_MODEL"           # e.g., llama3
# OLLAMA_BASE_URL="http://localhost:11434" # Default if Ollama runs locally

# --- RAG Configuration ---
# RETRIEVER_K=3 # Number of relevant document chunks to retrieve (default: 3)
# LLM_TEMPERATURE=0.7 # Controls randomness in LLM generation (e.g., 0.0=deterministic, 1.0=creative, default: 0.7)